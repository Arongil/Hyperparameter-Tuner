import numpy as np

# f1 is an example function to optimize. It has numerous maxima and minima.
def f1(v):
    # v is an 2D coordinate.
    return v[0]**2 / 2 + np.sin(v[1] / 12 + v[0]*v[1]/20)/50 - 2

# f2 is the 3D analogy of the Bell Curve. It has a maximum at (0, 0).
def f2(v):
    return np.exp(-(v[0]*v[0] + v[1]*v[1]))

# f3 is a quintic polynomial with maxima -3.644 and -1.456 and with minima -2.544 and -0.356.
def f3(v):
    return v[0] * (v[0] + 1) * (v[0] + 2) * (v[0] + 3) * (v[0] + 4)

# gradient computes the gradient of a function of n dimensions, f, at position v.
def gradient(f, v):
    grad = [0 for i in range(n)] # This is the gradient array of partial derivatives.
    h = 1e-8 # The limiting variable in the limit definition of the partial derivative is approximated with 1e-8.
    for i in range(len(v)):
        step = v[:]
        step[i] += h
        grad[i] = (f(step) - f(v)) / h # limit definition of the partial derivative
    return grad

# optimize uses gradient descent to minimize or maximize arbitrary multivariable functions. optimize should only take functions that don't explode.
# This means the function shouldn't have points at negative infinity if it's being minimized, for example.
def optimize(f, n, bound = 10, trials = 10, stepSize = 0.01, steps = 100, minimize = True):
    '''
    optimize takes a function (f), its dimension (n), the range in which it should initialize coordinates for optimization,
    how many trials to run in search for a global minimum (trials), how big steps should be (stepSize), how many times to run gradient descent (steps),
    and a boolean on whether to maximize or minimize (minimize). It applies gradient descent/ascent steps times to locate a critical point.
    It repeats this process trials times and returns the lowest/highest of the local minima/maxima.
    '''
    criticalPoints = [] # Stores local critical points as 1x2 arrays: [value, position].
    for trial in range(trials):
        # Initialize random coordinates within bound of the origin.
        coordinates = [(np.random.random() - 1/2) * 2*bound for i in range(n)]
        for step in range(steps):
            # Run steps iterations of gradient descent on the random coordinates.
            grad = gradient(f, coordinates)
            for i in range(len(coordinates)):
                if minimize:
                    coordinates[i] -= stepSize * grad[i]
                else:
                    coordinates[i] += stepSize * grad[i]
        criticalPoints.append(coordinates)
    globalOptimum = criticalPoints[0]
    optimumOutput = f(globalOptimum)
    for i in range(1, len(criticalPoints)):
        if minimize:
            if f(criticalPoints[i]) < optimumOutput:
                globalOptimum = criticalPoints[i]
                optimumOutput = f(globalOptimum)
        else:
            if f(criticalPoints[i]) > optimumOutput:
                globalOptimum = criticalPoints[i]
                optimumOutput = f(globalOptimum)
    return globalOptimum
